State=SVStateSpace[StateNumber,]
CurrentNode=which.min(State[1:n])
CurrentNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
Actions=vector(length=0)
for(Node in 1:n)
{
if(CurrentNodeRow[Node]==1)
{
Actions=c(Actions,Node)
}
}
for(ActionNumber in 1:NumberOfActionsFromState[StateNumber])
{
#For this state and this action store the cost in the objective function
if(StateNumber==1)
{
Objective[ActionNumber]=CostOfAction(State,Actions[ActionNumber],n,CostVec,xVec,LambdaVec)
}
else
{
Objective[sum(NumberOfActionsFromState[1:StateNumber-1])+ActionNumber]=CostOfAction(State,Actions[ActionNumber],n,CostVec,xVec,LambdaVec)
}
}
}
return(list(Objective=Objective,MatrixConstraints=ALHS,VectorBounds=bRHS,StateSpace=SVStateSpace,NumberOfActionsFromState=NumberOfActionsFromState))
}
SolveDualLP<-function(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec)
{
CreatedDual=CreateDualSetup(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec)
A=CreatedDual$MatrixConstraints
b=CreatedDual$VectorBounds
print(A)
print(b)
Objdir="min"
Objective=CreatedDual$Objective
Constdir=rep("=",nrow(A))
print("Starting to solve")
Solved=lp(Objdir,Objective,A,Constdir,b)
return(list(Value=Solved ,Solution=Solved$solution))
}
OptimalDualDesicionPolicy<-function(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec)
{
#Solve the Dual LP
CreatedDual=CreateDualSetup(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec)
A=CreatedDual$MatrixConstraints
b=CreatedDual$VectorBounds
SVStateSpace=CreatedDual$StateSpace
NumberOfActionsFromState=CreatedDual$NumberOfActionsFromState
print(A)
print(b)
Objdir="min"
Objective=CreatedDual$Objective
Constdir=rep("=",nrow(A))
print("Starting to solve")
Solved=lp(Objdir,Objective,A,Constdir,b)
Solution=Solved$solution
Value=Solved$objval
print("Solution found")
print(Solution)
print("For objective value")
print(Value)
#We now split the solution into x's and y's
NumberOfVariables=length(Solution)/2
OptimalX=Solution[1:NumberOfVariables]
print("Optimal x's are")
print(OptimalX)
OptimalY=Solution[NumberOfVariables+1:length(Solution)]
print("Optimal y's are")
print(OptimalY)
#From the optimal x/y's we create a decision rule
#First identify Recurrent states and transient states
#Record 1 if recurrent, 0 if transient
StateType=vector(length=nrow(SVStateSpace))
#OptimalDecision=list(length=nrow(SVStateSpace))
OptimalDecision=list()
for(StateNumber in 1:nrow(SVStateSpace))
{
State=SVStateSpace[StateNumber,]
CurrentNode=which.min(State[1:n])
CurrentActions=AdjMatrix[CurrentNode,]
NodesCanMoveTo=which(CurrentActions==1)
#Work out the sum of the x's
if(StateNumber==1)
{
SumOfX=sum(OptimalX[1:NumberOfActionsFromState[1]])
}
else
{
SumOfX=sum(OptimalX[(sum(NumberOfActionsFromState[1:(StateNumber-1)])+1):sum(NumberOfActionsFromState[1:StateNumber])])
}
print(SumOfX)
if(SumOfX>0)
{
#print("We have a recurrent State")
#If the sum is postive then this state is recurrent and put into S_{x}
StateType[StateNumber]=1
StatesOptimalDecisions=vector(length=0)
#We then run through and record all x>0
for(ActionNumber in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
if(OptimalX[ActionNumber]>0)
{
#Record it as an optimal decision
StatesOptimalDecisions=c(StatesOptimalDecisions,NodesCanMoveTo[ActionNumber])
}
}
else
{
if(OptimalX[sum(NumberOfActionsFromState[1:(StateNumber-1)])+ActionNumber]>0)
{
StatesOptimalDecisions=c(StatesOptimalDecisions,NodesCanMoveTo[ActionNumber])
}
}
}
OptimalDecision[StateNumber]=StatesOptimalDecisions
}
else
{
#print("We have a transient State")
#For transient states we look at y's
#If the sum is postive then this state is recurrent and put into S_{x}
StateType[StateNumber]=0
StatesOptimalDecisions=vector(length=0)
#We then run through and record all x>0
for(ActionNumber in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
if(OptimalY[ActionNumber]>0)
{
#Record it as an optimal decision
StatesOptimalDecisions=c(StatesOptimalDecisions,NodesCanMoveTo[ActionNumber])
}
}
else
{
if(OptimalY[sum(NumberOfActionsFromState[1:(StateNumber-1)])+ActionNumber]>0)
{
StatesOptimalDecisions=c(StatesOptimalDecisions,NodesCanMoveTo[ActionNumber])
}
}
}
OptimalDecision[StateNumber]=StatesOptimalDecisions
}
}
return(list(OptimalDecision=OptimalDecision,StateSpace=SVStateSpace))
}
OptimalDualDesicionPolicy(matrix(rep(1,9),nrow=3),3,c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
CreateDualSetup<-function(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec,SVStateSpace=NULL,AlphaVec=NULL)
{
BVec=ceiling(xVec)
if(is.null(SVStateSpace))
{
SVStateSpace=CreateSVStates(n,BVec,bVec)
}
if(is.null(AlphaVec))
{
AlphaVec=vector(length=nrow(SVStateSpace))
AlphaVec=rep(1/length(AlphaVec),length(AlphaVec))
}
#We now work out the number of x's and y's needed (i.e the size of (s,a))
NumberOfXVariables=0
NumberOfActionsFromState=vector(length=0)
for(state in 1:nrow(SVStateSpace))
{
#For each state we need to work out the number of actions
#First we work out the current node (and then how many actions can be taken)
StateVector=SVStateSpace[state,]
CurrentNode=which.min(StateVector[1:n])
AdjRow=AdjMatrix[CurrentNode,]
NumberOfXVariables=NumberOfXVariables+length(AdjRow[AdjRow==1])
NumberOfActionsFromState=c(NumberOfActionsFromState,length(AdjRow[AdjRow==1]))
}
NumberOfYVariables=NumberOfXVariables
NumberOfVariables=NumberOfXVariables+NumberOfYVariables
#creating the LHS matrix part A
ALHS=matrix(nrow=0,ncol=NumberOfVariables)
#storage for the RHS vector part b
bRHS=vector(length=0)
#FIRST DUAL CONSTRAINT
for(StateNumber in 1:nrow(SVStateSpace))
{
#We will now create the constraint for taking that action
ConstraintRow=vector(length=NumberOfVariables)
#We store our state we are working on the constraint row for
State=SVStateSpace[StateNumber,]
CurrentNode=which.min(State[1:n])
CurrentNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
Actions=vector(length=0)
for(Node in 1:n)
{
if(CurrentNodeRow[Node]==1)
{
Actions=c(Actions,Node)
}
}
#We place 1's in the all the actions possible for this state x
for(i in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
ConstraintRow[i]=ConstraintRow[i]+1
}
else
{
ConstraintRow[sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]=
ConstraintRow[sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]+1
}
}
#We now subtract prob(moving to state summed over all states and actions) for x
for(oldstatenumber in 1:nrow(SVStateSpace))
{
OldState=SVStateSpace[oldstatenumber,]
OldNode=which.min(OldState[1:n])
OldNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
OldActions=vector(length=0)
for(Node in 1:n)
{
if(OldNodeRow[Node]==1)
{
OldActions=c(OldActions,Node)
}
}
for(actionnumber in 1:NumberOfActionsFromState[oldstatenumber])
{
#Using this oldstate and action , is it possible that the new state is the current working state
New=NewSVState(OldState,OldActions[actionnumber],BVec,bVec,LambdaVec)
NewState=New$State
NewStateProb=New$Prob
for(NewStateNumber in 1:nrow(NewState))
{
if(all(NewState[NewStateNumber,]==SVStateSpace[StateNumber,]))
{
#If we can possibly move to the state we are working on we subtract the probability
if(oldstatenumber==1)
{
ConstraintRow[actionnumber]=ConstraintRow[actionnumber]-NewStateProb[NewStateNumber]
}
else
{
ConstraintRow[sum(NumberOfActionsFromState[1:(oldstatenumber-1)])+actionnumber]=
ConstraintRow[sum(NumberOfActionsFromState[1:(oldstatenumber-1)])+actionnumber]-NewStateProb[NewStateNumber]
}
}
}
}
}
ALHS=rbind(ALHS,ConstraintRow)
bRHS=c(bRHS,0)
}
#SECOND DUAL CONSTRAINT
for(StateNumber in 1:nrow(SVStateSpace))
{
#We will now create the constraint for taking that action
ConstraintRow=vector(length=NumberOfVariables)
#We store our state we are working on the constraint row for
State=SVStateSpace[StateNumber,]
CurrentNode=which.min(State[1:n])
CurrentNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
Actions=vector(length=0)
for(Node in 1:n)
{
if(CurrentNodeRow[Node]==1)
{
Actions=c(Actions,Node)
}
}
#We place 1's in the all the actions possible for this state for x
for(i in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
ConstraintRow[i]=ConstraintRow[i]+1
}
else
{
ConstraintRow[sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]=
ConstraintRow[sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]+1
}
}
#We place 1's in the all the actions possible for this state for y
for(i in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
ConstraintRow[NumberOfXVariables+i]=ConstraintRow[NumberOfXVariables+i]+1
}
else
{
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]=
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]+1
}
}
#We now subtract prob(moving to state summed over all states and actions) for y
for(oldstatenumber in 1:nrow(SVStateSpace))
{
OldState=SVStateSpace[oldstatenumber,]
OldNode=which.min(OldState[1:n])
OldNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
OldActions=vector(length=0)
for(Node in 1:n)
{
if(OldNodeRow[Node]==1)
{
OldActions=c(OldActions,Node)
}
}
for(actionnumber in 1:NumberOfActionsFromState[oldstatenumber])
{
#Using this oldstate and action , is it possible that the new state is the current working state
New=NewSVState(OldState,OldActions[actionnumber],BVec,bVec,LambdaVec)
NewState=New$State
NewStateProb=New$Prob
for(NewStateNumber in 1:nrow(NewState))
{
if(all(NewState[NewStateNumber,]==SVStateSpace[StateNumber,]))
{
#If we can possibly move to the state we are working on we subtract the probability
if(oldstatenumber==1)
{
ConstraintRow[NumberOfXVariables+actionnumber]=ConstraintRow[NumberOfXVariables+actionnumber]-NewStateProb[NewStateNumber]
}
else
{
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(oldstatenumber-1)])+actionnumber]=
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(oldstatenumber-1)])+actionnumber]-NewStateProb[NewStateNumber]
}
}
}
}
}
ALHS=rbind(ALHS,ConstraintRow)
bRHS=c(bRHS,AlphaVec[StateNumber])
}
Objective=vector(length=NumberOfVariables)
#We now create the Objective
for(StateNumber in 1:nrow(SVStateSpace))
{
#We store our state we are working on the constraint row for
State=SVStateSpace[StateNumber,]
CurrentNode=which.min(State[1:n])
CurrentNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
Actions=vector(length=0)
for(Node in 1:n)
{
if(CurrentNodeRow[Node]==1)
{
Actions=c(Actions,Node)
}
}
for(ActionNumber in 1:NumberOfActionsFromState[StateNumber])
{
#For this state and this action store the cost in the objective function
if(StateNumber==1)
{
Objective[ActionNumber]=CostOfAction(State,Actions[ActionNumber],n,CostVec,xVec,LambdaVec)
}
else
{
Objective[sum(NumberOfActionsFromState[1:StateNumber-1])+ActionNumber]=CostOfAction(State,Actions[ActionNumber],n,CostVec,xVec,LambdaVec)
}
}
}
return(list(Objective=Objective,MatrixConstraints=ALHS,VectorBounds=bRHS,StateSpace=SVStateSpace,NumberOfActionsFromState=NumberOfActionsFromState))
}
SolveDualLP<-function(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec)
{
CreatedDual=CreateDualSetup(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec)
A=CreatedDual$MatrixConstraints
b=CreatedDual$VectorBounds
print(A)
print(b)
Objdir="min"
Objective=CreatedDual$Objective
Constdir=rep("=",nrow(A))
print("Starting to solve")
Solved=lp(Objdir,Objective,A,Constdir,b)
return(list(Value=Solved ,Solution=Solved$solution))
}
OptimalDualDesicionPolicy<-function(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec)
{
#Solve the Dual LP
CreatedDual=CreateDualSetup(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec)
A=CreatedDual$MatrixConstraints
b=CreatedDual$VectorBounds
SVStateSpace=CreatedDual$StateSpace
NumberOfActionsFromState=CreatedDual$NumberOfActionsFromState
print(A)
print(b)
Objdir="min"
Objective=CreatedDual$Objective
Constdir=rep("=",nrow(A))
print("Starting to solve")
Solved=lp(Objdir,Objective,A,Constdir,b)
Solution=Solved$solution
Value=Solved$objval
print("Solution found")
print(Solution)
print("For objective value")
print(Value)
#We now split the solution into x's and y's
NumberOfVariables=length(Solution)/2
OptimalX=Solution[1:NumberOfVariables]
print("Optimal x's are")
print(OptimalX)
OptimalY=Solution[(NumberOfVariables+1):length(Solution)]
print("Optimal y's are")
print(OptimalY)
#From the optimal x/y's we create a decision rule
#First identify Recurrent states and transient states
#Record 1 if recurrent, 0 if transient
StateType=vector(length=nrow(SVStateSpace))
#OptimalDecision=list(length=nrow(SVStateSpace))
OptimalDecision=list()
for(StateNumber in 1:nrow(SVStateSpace))
{
State=SVStateSpace[StateNumber,]
CurrentNode=which.min(State[1:n])
CurrentActions=AdjMatrix[CurrentNode,]
NodesCanMoveTo=which(CurrentActions==1)
#Work out the sum of the x's
if(StateNumber==1)
{
SumOfX=sum(OptimalX[1:NumberOfActionsFromState[1]])
}
else
{
SumOfX=sum(OptimalX[(sum(NumberOfActionsFromState[1:(StateNumber-1)])+1):sum(NumberOfActionsFromState[1:StateNumber])])
}
print(SumOfX)
if(SumOfX>0)
{
#print("We have a recurrent State")
#If the sum is postive then this state is recurrent and put into S_{x}
StateType[StateNumber]=1
StatesOptimalDecisions=vector(length=0)
#We then run through and record all x>0
for(ActionNumber in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
if(OptimalX[ActionNumber]>0)
{
#Record it as an optimal decision
StatesOptimalDecisions=c(StatesOptimalDecisions,NodesCanMoveTo[ActionNumber])
}
}
else
{
if(OptimalX[sum(NumberOfActionsFromState[1:(StateNumber-1)])+ActionNumber]>0)
{
StatesOptimalDecisions=c(StatesOptimalDecisions,NodesCanMoveTo[ActionNumber])
}
}
}
OptimalDecision[StateNumber]=StatesOptimalDecisions
}
else
{
#print("We have a transient State")
#For transient states we look at y's
#If the sum is postive then this state is recurrent and put into S_{x}
StateType[StateNumber]=0
StatesOptimalDecisions=vector(length=0)
#We then run through and record all x>0
for(ActionNumber in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
if(OptimalY[ActionNumber]>0)
{
#Record it as an optimal decision
StatesOptimalDecisions=c(StatesOptimalDecisions,NodesCanMoveTo[ActionNumber])
}
}
else
{
if(OptimalY[sum(NumberOfActionsFromState[1:(StateNumber-1)])+ActionNumber]>0)
{
StatesOptimalDecisions=c(StatesOptimalDecisions,NodesCanMoveTo[ActionNumber])
}
}
}
OptimalDecision[StateNumber]=StatesOptimalDecisions
}
}
return(list(OptimalDecision=OptimalDecision,StateSpace=SVStateSpace))
}
OptimalDualDesicionPolicy(matrix(rep(1,9),nrow=3),3,c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
OptimalDualDesicionPolicy(matrix(rep(1,9),nrow=3),3,c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
library(gtools)
library(utils)
library(lpSolve)
OptimalDualDesicionPolicy(matrix(rep(1,9),nrow=3),3,c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
ValueIterationForGame(1000,0.0001,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
OptimalDualDesicionPolicy(matrix(rep(1,9),nrow=3),3,c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
ValueIterationForGame(1000,0.0001,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
ActiveConstraintsExperimental(matrix(rep(1,9),nrow=3),3,c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
ActiveConstraintsExperimental(matrix(rep(1,9),nrow=3),3,c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))[5,]
ActiveConstraintsExperimental(matrix(rep(1,9),nrow=3),3,c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
ValueIterationForGame(1000,0.0001,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
OptimalDualDesicionPolicy(matrix(rep(1,9),nrow=3),3,c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
ValueIterationForGame(1000,0.0001,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
ActiveConstraintsExperimental(matrix(rep(1,9),nrow=3),3,c(1.2,1.8,2.2),rep(0,3),rep(1,3),rep(1/3,3))
