NewMeanVState<-function(CurrentVState,NewSState,NodeMovedTo,BVec,bVec,lambdaVec)
{
print("I am about to evolve to the mean v state")
NewV=(NewVState(CurrentVState,NewSState,NodeMovedTo,BVec,bVec,lambdaVec)$State)[1,]
print(NewV)
NewV[NodeMovedTo]=TruncPoissionMean(lambdaVec[NodeMovedTo],bVec[NodeMovedTo])
return(NewV)
}
SimulationForEvolution(2,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
#NOTE. This function does not return an actual possible state, but a mean state
NewMeanVState<-function(CurrentVState,NewSState,NodeMovedTo,BVec,bVec,lambdaVec)
{
print("I am about to evolve to the mean v state")
NewV=(NewVState(CurrentVState,NewSState,NodeMovedTo,BVec,bVec,lambdaVec)$State)[1,]
print(NewV)
NewV[NodeMovedTo]=TruncPoissionMean(lambdaVec[NodeMovedTo],bVec[NodeMovedTo])
return(NewV)
}
SimulationForEvolution(2,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
#Evolution of V States function
#note. We need to know the current S state , as if one value is B+2, then v is set to 0
NewVState<-function(CurrentVState,NewSState,NodeMovedTo,BVec,bVec,lambdaVec)
{
#We aim to store the New V states and the probability of ending up in one.
NewVState=matrix(nrow=(bVec[NodeMovedTo]+1),ncol=length(CurrentVState))
NewVStateProb=vector(length=(bVec[NodeMovedTo]+1))
WorkingVState=CurrentVState
#We now need to set any v=0 if there s=B+1
for(i in 1:length(NewSState))
{
if(NewSState[i]==BVec[i]+1)
{
WorkingVState[i]=0
}
}
stopifnot(length(WorkingVState)==ncol(NewVState))
for(i in 1:(bVec[NodeMovedTo]+1))
{
#copy the current state
print("hello")
NewVState[i,]=WorkingVState
#Store the new evolved value
NewVState[i,NodeMovedTo]=(i-1)
#Also store the probability in a vector
NewVStateProb[i]=TruncPoissonPMF(lambdaVec[NodeMovedTo],bVec[NodeMovedTo],i-1)
}
return(list(State=NewVState,Prob=NewVStateProb))
}
SimulationForEvolution(2,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
#Evolution of V States function
#note. We need to know the current S state , as if one value is B+2, then v is set to 0
NewVState<-function(CurrentVState,NewSState,NodeMovedTo,BVec,bVec,lambdaVec)
{
#We aim to store the New V states and the probability of ending up in one.
NewVState=matrix(nrow=(bVec[NodeMovedTo]+1),ncol=length(CurrentVState))
NewVStateProb=vector(length=(bVec[NodeMovedTo]+1))
WorkingVState=CurrentVState
#We now need to set any v=0 if there s=B+1
for(i in 1:length(NewSState))
{
if(NewSState[i]==BVec[i]+1)
{
WorkingVState[i]=0
}
}
print(WorkingVState)
print(NewVState)
stopifnot(length(WorkingVState)==ncol(NewVState))
for(i in 1:(bVec[NodeMovedTo]+1))
{
#copy the current state
print("hello")
NewVState[i,]=WorkingVState
#Store the new evolved value
NewVState[i,NodeMovedTo]=(i-1)
#Also store the probability in a vector
NewVStateProb[i]=TruncPoissonPMF(lambdaVec[NodeMovedTo],bVec[NodeMovedTo],i-1)
}
return(list(State=NewVState,Prob=NewVStateProb))
}
SimulationForEvolution(2,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
#Evolution of V States function
#note. We need to know the current S state , as if one value is B+2, then v is set to 0
NewVState<-function(CurrentVState,NewSState,NodeMovedTo,BVec,bVec,lambdaVec)
{
print(CurrentVState)
#We aim to store the New V states and the probability of ending up in one.
NewVState=matrix(nrow=(bVec[NodeMovedTo]+1),ncol=length(CurrentVState))
NewVStateProb=vector(length=(bVec[NodeMovedTo]+1))
WorkingVState=CurrentVState
#We now need to set any v=0 if there s=B+1
for(i in 1:length(NewSState))
{
if(NewSState[i]==BVec[i]+1)
{
WorkingVState[i]=0
}
}
print(WorkingVState)
print(NewVState)
stopifnot(length(WorkingVState)==ncol(NewVState))
for(i in 1:(bVec[NodeMovedTo]+1))
{
#copy the current state
print("hello")
NewVState[i,]=WorkingVState
#Store the new evolved value
NewVState[i,NodeMovedTo]=(i-1)
#Also store the probability in a vector
NewVStateProb[i]=TruncPoissonPMF(lambdaVec[NodeMovedTo],bVec[NodeMovedTo],i-1)
}
return(list(State=NewVState,Prob=NewVStateProb))
}
SimulationForEvolution(2,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
SimulationForEvolution(2,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
SimulateVStateEvolution<-function(NodeToEvolve,NewsVec,vVec,xVec,LambdaVec,bVec)
{
BVec=ceiling(xVec)
NewvVec=(NewVState(vVec,NewsVec,NodeToEvolve,BVec,bVec,LambdaVec)$State)[1]
print("Simulated V Vector is")
print(NewvVec)
NewvVec[NodeToEvolve]=min(bVec[NodeToEvolve],rpois(1,LambdaVec[NodeToEvolve]))
return(NewvVec)
}
SimulationForEvolution(2,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
SimulateVStateEvolution<-function(NodeToEvolve,NewsVec,vVec,xVec,LambdaVec,bVec)
{
BVec=ceiling(xVec)
NewvVec=(NewVState(vVec,NewsVec,NodeToEvolve,BVec,bVec,LambdaVec)$State)[1,]
print("Simulated V Vector is")
print(NewvVec)
NewvVec[NodeToEvolve]=min(bVec[NodeToEvolve],rpois(1,LambdaVec[NodeToEvolve]))
return(NewvVec)
}
SimulationForEvolution(2,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
SimulationForEvolution(10,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
SolveLP(matrix(rep(1,16),ncol=4,nrow=4),4,rep(1.1,4),rep(2,4),rep(1,4),rep(1,4))
SimulationForEvolution(10,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
SimulationExperiment(10,100,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
source("Heuristics.R")
library(stats)
library(ggplot2)
library(reshape2)
SimulateVStateEvolution<-function(NodeToEvolve,NewsVec,vVec,xVec,LambdaVec,bVec)
{
BVec=ceiling(xVec)
NewvVec=(NewVState(vVec,NewsVec,NodeToEvolve,BVec,bVec,LambdaVec)$State)[1,]
NewvVec[NodeToEvolve]=min(bVec[NodeToEvolve],rpois(1,LambdaVec[NodeToEvolve]))
return(NewvVec)
}
SimulationForEvolution<-function(NumberOfRunSteps,HeuristicDepth,HeuristicFunction,n,AdjacencyMatrix,IndexForNodeFunction,CostVec,LambdaVec,bVec,xVec,vMaxVec=NULL)
{
BVec=ceiling(xVec)
if(is.null(vMaxVec))
{
#Create vMaxVec
vMaxVec=CreateVMaxVector(n,LambdaVec,bVec,xVec)
}
RunCost=vector(length=NumberOfRunSteps)
for(run in 0:NumberOfRunSteps)
{
if(run==0)
{
#Set up initial States
StartNode=StartingNodeHeuristic(n,IndexForNodeFunction,CostVec,LambdaVec,bVec,xVec,vMaxVec=NULL)
#print(paste("Starting at ",toString(StartNode)))
sVec=BVec+1
sVec[StartNode]=1
vVec=vector(length=n)
for(i in 1:n)
{
vVec[i]=TruncPoissionMean(LambdaVec[i],bVec[i])
}
}
else
{
#Perform a run
OldsVec=sVec
OldvVec=vVec
#decide where to move to using heuristic
MoveToNode=HeuristicFunction(HeuristicDepth,n,AdjacencyMatrix,IndexForNodeFunction,OldsVec,OldvVec,CostVec,LambdaVec,bVec,xVec,vMaxVec)
#print(MoveToNode)
#Calculate cost of doing such an action
RunCost[run]=CostOfAction(c(OldsVec,OldvVec),MoveToNode,n,CostVec,xVec,LambdaVec)
#Evolve System
sVec=NewSState(OldsVec,MoveToNode,BVec)
vVec=SimulateVStateEvolution(MoveToNode,sVec,OldvVec,xVec,LambdaVec,bVec)
#print(paste("Moved to ",toString(MoveToNode),"Costing ",toString(RunCost[run])))
}
}
AverageCost=sum(RunCost)/NumberOfRunSteps
print(paste("Average cost is ",toString(AverageCost)))
return(list(Average=AverageCost,CostForStep=RunCost))
}
SimulationExperiment<-function(NumberOfTrials,NumberOfRunSteps,HeuristicDepth,HeuristicFunction,n,AdjacencyMatrix,IndexForNodeFunction,CostVec,LambdaVec,bVec,xVec,vMaxVec=NULL)
{
#We repeat the simulation
RunningCostMatrix=matrix(0,nrow = 0,ncol=NumberOfRunSteps)
AveragecostVec=vector(length=NumberOfTrials)
for(trial in 1:NumberOfTrials)
{
Simulation=SimulationForEvolution(NumberOfRunSteps,HeuristicDepth,HeuristicFunction,n,AdjacencyMatrix,IndexForNodeFunction,CostVec,LambdaVec,bVec,xVec)
RunningCostMatrix=rbind(RunningCostMatrix,Simulation$CostForStep)
AveragecostVec[trial]=Simulation$Average
#print(RunningCostMatrix)
}
AverageAmongSimulations=sum(AveragecostVec)/NumberOfTrials
return(AverageAmongSimulations)
}
SimulationExperiment(10,100,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
source("Index Implementation.R")
#One step Benefit Heuristic
#This function decides given the current state to look at the available actions and pick the action that produces the highest index
OneStepBenefitHeuristic<-function(n,AdjacencyMatrix,IndexForNodeFunction,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec=NULL)
{
if(is.null(vMaxVec))
{
#Create vMaxVec
vMaxVec=CreateVMaxVector(n,LambdaVec,bVec,xVec)
}
#Get indices
NodeIndexes=IndicesForNodes(n,IndexForNodeFunction,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec)
#print(NodeIndexes)
#Identify which state we are at by
CurrentNode=which.min(sVec)
#Actions available
Actions=AdjacencyMatrix[CurrentNode,]
IndexForActions=Actions * NodeIndexes
#Find best action
BestAction=which.max(IndexForActions)
return(BestAction)
}
OneStepPenaltyHeuristic<-function(n,AdjacencyMatrix,IndexForNodeFunction,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec=NULL)
{
if(is.null(vMaxVec))
{
#Create vMaxVec
vMaxVec=CreateVMaxVector(n,LambdaVec,bVec,xVec)
}
#Get indices
NodeIndexes=IndicesForNodes(n,IndexForNodeFunction,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec)
#Identify which state we are at by
CurrentNode=which.min(sVec)
#Actions available
Actions=AdjacencyMatrix[CurrentNode,]
IndexForActions=Actions * NodeIndexes
PenaltyForActions=rep(sum(IndexForActions),length(Actions))-IndexForActions
#print(PenaltyForActions)
#Find best action
BestAction=which.min(PenaltyForActions)
return(BestAction)
}
DeterministicCostEvaluationOfPath<-function(Path,n,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec=NULL)
{
BVec=ceiling(xVec)
#We follow the path for as many steps as it has
i=1
SumOfCost=0
while(Path[i]!=0 && i<=length(Path))
{
#Add actions cost
SumOfCost=SumOfCost+CostOfAction(c(sVec,vVec),Path[i],n,CostVec,xVec,LambdaVec)
#Evolve DETERMINISTICALLY
sVec=NewSState(sVec,Path[i],BVec)
vVec=NewMeanVState(vVec,sVec,Path[i],BVec,bVec,LambdaVec)
i=i+1
}
return(list(Average=SumOfCost/(i-1),Overall=SumOfCost))
}
#Here we sum the indices up over the number of steps for all paths of the Number of steps
MultiStepBenefitHeuristic<-function(NoSteps,n,AdjacencyMatrix,IndexForNodeFunction,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec=NULL)
{
if(is.null(vMaxVec))
{
#Create vMaxVec
vMaxVec=CreateVMaxVector(n,LambdaVec,bVec,xVec)
}
BVec=ceiling(xVec)
Paths=matrix(0,nrow=0,ncol=NoSteps)
#Note. We will be using mean v Evolution for use with the index
EvolvedStates=matrix(0,nrow=0,ncol=2*n)
BenefitForPath=vector(length=0)
BestPathforStep=matrix(0,nrow=NoSteps,ncol=NoSteps)
for(Step in 1:NoSteps)
{
if(Step==1)
{
#We run the initial set up
CurrentNode=which.min(sVec)
#Actions
Actions=AdjacencyMatrix[CurrentNode,]
NodeIndexes=IndicesForNodes(n,IndexForNodeFunction,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec)
BenefitForAction=Actions * NodeIndexes
#Form Paths
for(action in 1:n)
{
if(Actions[action]==1)
{
Paths=rbind(Paths,c(action,rep(0,NoSteps-1)))
NewsVec=NewSState(sVec,action,BVec)
EvolvedStates=rbind(EvolvedStates,c(NewsVec,NewMeanVState(vVec,NewsVec,action,BVec,bVec,LambdaVec)))
BenefitForPath=c(BenefitForPath,BenefitForAction[action])
}
}
BestPath=Paths[which.max(BenefitForPath),]
BestPathforStep[Step,]=BestPath
}
else
{
#We need to copy and expand each
OldPaths=Paths
OldEvolvedStates=EvolvedStates
OldBenefitForPath=BenefitForPath
Paths=matrix(0,nrow=0,ncol=NoSteps)
EvolvedStates=matrix(0,nrow=0,ncol=2*n)
BenefitForPath=vector(length=0)
#We now look at expanding each
for(row in 1:nrow(OldPaths))
{
#for each row we expand it to allow all possible actions
Actions=AdjacencyMatrix[OldPaths[row,Step-1],]
NodeIndexes=IndicesForNodes(n,IndexForNodeFunction,OldEvolvedStates[row,1:n],OldEvolvedStates[row,(n+1):(2*n)],CostVec,LambdaVec,bVec,xVec,vMaxVec)
BenefitForAction=Actions * NodeIndexes
#Form Paths
for(action in 1:n)
{
if(Actions[action]==1)
{
Paths=rbind(Paths,c(OldPaths[row,1:(Step-1)],action,rep(0,NoSteps-Step)))
NewsVec=NewSState(OldEvolvedStates[row,1:n],action,BVec)
EvolvedStates=rbind(EvolvedStates,c(NewsVec,NewMeanVState(OldEvolvedStates[row,(n+1):(2*n)],NewsVec,action,BVec,bVec,LambdaVec)))
BenefitForPath=c(BenefitForPath,OldBenefitForPath[row]+BenefitForAction[action])
}
}
}
BestPath=Paths[which.max(BenefitForPath),]
BestPathforStep[Step,]=BestPath
}
}
#For each look ahead step we have a path
#print(BestPathforStep)
AverageCostforPath=vector(length=NoSteps)
#We now need to see how good they perform
for(i in 1:NoSteps)
{
#We compute the average cost of following such a strategy to decide which paths to pick
#We use determinsitic evolution to the mean state in v
AverageCostforPath[i]=DeterministicCostEvaluationOfPath(BestPathforStep[i,],n,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec)$Average
}
#print(AverageCostforPath)
OverallBestPath=BestPathforStep[which.min(AverageCostforPath),]
return(OverallBestPath[1])
}
#Here we sum the indices up over the number of steps for all paths of the Number of steps
MultiStepPenaltyHeuristic<-function(NoSteps,n,AdjacencyMatrix,IndexForNodeFunction,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec=NULL)
{
if(is.null(vMaxVec))
{
#Create vMaxVec
vMaxVec=CreateVMaxVector(n,LambdaVec,bVec,xVec)
}
BVec=ceiling(xVec)
Paths=matrix(0,nrow=0,ncol=NoSteps)
#Note. We will be using mean v Evolution for use with the index
EvolvedStates=matrix(0,nrow=0,ncol=2*n)
PenaltyForPath=vector(length=0)
BestPathforStep=matrix(0,nrow=NoSteps,ncol=NoSteps)
for(Step in 1:NoSteps)
{
if(Step==1)
{
#We run the initial set up
CurrentNode=which.min(sVec)
#Actions
Actions=AdjacencyMatrix[CurrentNode,]
NodeIndexes=IndicesForNodes(n,IndexForNodeFunction,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec)
IndexForActions= Actions * NodeIndexes
PenaltyForAction=rep(sum(IndexForActions),length(Actions))-IndexForActions
#Form Paths
for(action in 1:n)
{
if(Actions[action]==1)
{
Paths=rbind(Paths,c(action,rep(0,NoSteps-1)))
NewsVec=NewSState(sVec,action,BVec)
EvolvedStates=rbind(EvolvedStates,c(NewsVec,NewMeanVState(vVec,NewsVec,action,BVec,bVec,LambdaVec)))
PenaltyForPath=c(PenaltyForPath,PenaltyForAction[action])
}
}
BestPath=Paths[which.min(PenaltyForPath),]
BestPathforStep[Step,]=BestPath
}
else
{
#We need to copy and expand each
OldPaths=Paths
OldEvolvedStates=EvolvedStates
OldPenaltyForPath=PenaltyForPath
Paths=matrix(0,nrow=0,ncol=NoSteps)
EvolvedStates=matrix(0,nrow=0,ncol=2*n)
PenaltyForPath=vector(length=0)
#We now look at expanding each
for(row in 1:nrow(OldPaths))
{
#for each row we expand it to allow all possible actions
Actions=AdjacencyMatrix[OldPaths[row,Step-1],]
NodeIndexes=IndicesForNodes(n,IndexForNodeFunction,OldEvolvedStates[row,1:n],OldEvolvedStates[row,(n+1):(2*n)],CostVec,LambdaVec,bVec,xVec,vMaxVec)
IndexForActions= Actions * NodeIndexes
PenaltyForAction=rep(sum(IndexForActions),length(Actions))-IndexForActions
#Form Paths
for(action in 1:n)
{
if(Actions[action]==1)
{
Paths=rbind(Paths,c(OldPaths[row,1:(Step-1)],action,rep(0,NoSteps-Step)))
NewsVec=NewSState(OldEvolvedStates[row,1:n],action,BVec)
EvolvedStates=rbind(EvolvedStates,c(NewsVec,NewMeanVState(OldEvolvedStates[row,(n+1):(2*n)],NewsVec,action,BVec,bVec,LambdaVec)))
PenaltyForPath=c(PenaltyForPath,OldPenaltyForPath[row]+PenaltyForAction[action])
}
}
}
BestPath=Paths[which.min(PenaltyForPath),]
BestPathforStep[Step,]=BestPath
}
}
#For each look ahead step we have a path
#print(BestPathforStep)
AverageCostforPath=vector(length=NoSteps)
#We now need to see how good they perform
for(i in 1:NoSteps)
{
#We compute the average cost of following such a strategy to decide which paths to pick
#We use determinsitic evolution to the mean state in v
AverageCostforPath[i]=DeterministicCostEvaluationOfPath(BestPathforStep[i,],n,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec)$Average
}
#print(AverageCostforPath)
OverallBestPath=BestPathforStep[which.min(AverageCostforPath),]
return(OverallBestPath[1])
}
StartingNodeHeuristic<-function(n,IndexForNodeFunction,CostVec,LambdaVec,bVec,xVec,vMaxVec=NULL)
{
if(is.null(vMaxVec))
{
#Create vMaxVec
vMaxVec=CreateVMaxVector(n,LambdaVec,bVec,xVec)
}
#Elapsed state, we will consider the indices when we have not visited in a very long time.
StartingSVec=ceiling(bVec)+1
#and we will assume that all v's are in their mean state.
StartingVVec=vector(length=n)
for(i in 1:n)
{
StartingVVec[i]=TruncPoissionMean(LambdaVec[i],bVec[i])
}
#To decide a starting node we work out the index for all nodes (when S is maximum) and pick the biggest
NodeIndexes=IndicesForNodes(n,IndexForNodeFunction,StartingSVec,StartingVVec,CostVec,LambdaVec,bVec,xVec,vMaxVec)
BestStart=which.max(NodeIndexes)
return(BestStart)
}
SimulationExperiment(10,100,4,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
SimulationExperiment(10,100,2,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
SimulationExperiment(10,1000,2,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
SimulationExperiment(10,1000,2,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(2,2,2,2),c(1.1,1.1,1.1,1.1))
SimulationExperiment(10,1000,2,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(0,0,0,0),c(1.1,1.1,1.1,1.1))
SolveLP(matrix(rep(1,16),ncol=4,nrow=4),4,rep(1.1,4),rep(0,4),rep(1,4),rep(1,4))
SolveLP(matrix(rep(1,16),ncol=4,nrow=4),4,rep(1.1,4),rep(3,4),rep(1,4),rep(1,4))
SolveLP(matrix(rep(1,16),ncol=4,nrow=4),4,rep(1.1,4),rep(2,4),rep(1,4),rep(1,4))
SolveLP(matrix(rep(1,16),ncol=4,nrow=4),4,rep(1.1,4),rep(1,4),rep(1,4),rep(1,4))
SolveLP(matrix(rep(1,16),ncol=4,nrow=4),4,rep(1.1,4),rep(0,4),rep(1,4),rep(1,4))
SolveLP(matrix(rep(1,16),ncol=4,nrow=4),4,rep(1.1,4),rep(0,4),rep(1,4),rep(1,4))
SimulationExperiment(10,1000,2,MultiStepBenefitHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(0,0,0,0),c(1.1,1.1,1.1,1.1))
SimulationExperiment(10,1000,2,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(1,1,1,1),c(1,1,1,1),c(1,1,1,1),c(1.1,1.1,1.1,1.1))
SolveLP(matrix(rep(1,16),ncol=4,nrow=4),4,rep(1.1,4),rep(0,4),rep(2,4),rep(2,4))
SimulationExperiment(10,1000,2,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(2,2,2,2),c(2,2,2,2),c(1,1,1,1),c(1.1,1.1,1.1,1.1))
DeterministicCostEvaluationOfPath<-function(Path,n,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec=NULL)
{
BVec=ceiling(xVec)
#We follow the path for as many steps as it has
i=1
SumOfCost=0
while(Path[i]!=0 && i<=length(Path))
{
#Add actions cost
SumOfCost=SumOfCost+CostOfAction(c(sVec,vVec),Path[i],n,CostVec,xVec,LambdaVec)
#Evolve DETERMINISTICALLY
sVec=NewSState(sVec,Path[i],BVec)
#vVec=NewMeanVState(vVec,sVec,Path[i],BVec,bVec,LambdaVec)
vVec=vector(0,length=n)
i=i+1
}
return(list(Average=SumOfCost/(i-1),Overall=SumOfCost))
}
SimulationExperiment(10,1000,2,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(2,2,2,2),c(2,2,2,2),c(1,1,1,1),c(1.1,1.1,1.1,1.1))
DeterministicCostEvaluationOfPath<-function(Path,n,sVec,vVec,CostVec,LambdaVec,bVec,xVec,vMaxVec=NULL)
{
BVec=ceiling(xVec)
#We follow the path for as many steps as it has
i=1
SumOfCost=0
while(Path[i]!=0 && i<=length(Path))
{
#Add actions cost
SumOfCost=SumOfCost+CostOfAction(c(sVec,vVec),Path[i],n,CostVec,xVec,LambdaVec)
#Evolve DETERMINISTICALLY
sVec=NewSState(sVec,Path[i],BVec)
#vVec=NewMeanVState(vVec,sVec,Path[i],BVec,bVec,LambdaVec)
vVec=rep(0,n)
i=i+1
}
return(list(Average=SumOfCost/(i-1),Overall=SumOfCost))
}
SimulationExperiment(10,1000,2,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4,ncol=4,byrow=TRUE),EqualStepIndexForNode,c(2,2,2,2),c(2,2,2,2),c(1,1,1,1),c(1.1,1.1,1.1,1.1))
