else if(sVec[i]==BVec[i])
{
sVec[i]=sVec[i]+1
vVec[i]=0
}
else
{
vVec[i]=0
}
}
return(c(sVec,vVec))
}
#Function to compare two policies (note. Assumed in same order as SVStateSpace)
ComparePolicies<-function(Policy1,Policy2)
{
#We assume the Policies have the format of a vector of lists
AgreeAt=vector(length=length(Policy1))
for(i in 1:length(Policy1))
{
print(Policy1[[i]])
print(Policy2[[i]])
if(all.equal(Policy1[[i]],Policy2[[i]]))
{
AgreeAt[i]=1
}
}
return(AgreeAt)
}
#New state if current state is neglected
NewSVStateIfNeglect<-function(CurrentSVState,BVec,bVec,lambdaVec)
{
#Note. We cannot simply use the prior functions
n=length(CurrentSVState)/2
sVec=CurrentSVState[1:n]
vVec=CurrentSVState[(n+1):(2*n)]
#Now we evolve sVec , by increasing it to cap, if its capped we removed the observed info
for(i in 1:n)
{
if(sVec[i] < BVec[i])
{
sVec[i]=sVec[i]+1
}
else if(sVec[i]==BVec[i])
{
sVec[i]=sVec[i]+1
vVec[i]=0
}
else
{
vVec[i]=0
}
}
return(c(sVec,vVec))
}
ComparePolicies(PolicyByDual,PolicyByHeuristic)
all.equal(1==3)
all.equal(1,3)
all.equal(c(1,2),c(2,2))
#Function to compare two policies (note. Assumed in same order as SVStateSpace)
ComparePolicies<-function(Policy1,Policy2)
{
#We assume the Policies have the format of a vector of lists
AgreeAt=vector(length=length(Policy1))
for(i in 1:length(Policy1))
{
print(Policy1[[i]])
print(Policy2[[i]])
if(all(Policy1[[i]]==Policy2[[i]]))
{
AgreeAt[i]=1
}
}
return(AgreeAt)
}
all.equal(c(1,2),c(2,2))
ComparePolicies(PolicyByDual,PolicyByHeuristic)
#Function to work out the value for a particular number of steps
#Expects states to be passed as a matrix (with rows being a state with s_1 , s_2,...,s_n,v_1,...,v_2)
ValueFunctionForScenario<-function(Steps,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec,Scenario,ScenarioTracking=NULL,PriorValueFunction=NULL,PriorActionsMatrix=NULL)
{
n=ncol(StateSpace)/2
if(is.null(ScenarioTracking))
{
ScenarioTracking=rep(1,n)
}
StateSpaceSize=nrow(StateSpace)
n=nrow(AdjacencyMatrix)
BVec=ceiling(xVec)
#Stores the value of this iteration
ValueVector=rep(0,StateSpaceSize)
#Store a list of actions for this iteration
AddOnActionsMatrix=matrix(list(),nrow=1,ncol=StateSpaceSize)
if(Steps==0) #Base case
{
return(list(Values=ValueVector,Actions=AddOnActionsMatrix))
}
else if(Steps!=0)
{
#Work out previous step
if(is.null(PriorValueFunction) || is.null(PriorActionsMatrix))
{
Prior=ValueFunction(Steps-1,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec)
PriorValueFunction=Prior$Values
PriorActionsMatrix=Prior$Actions
}
#Form a vector from which to take the minimum for all states
for(state in 1:StateSpaceSize)
{
#Current state is
CurrentState=StateSpace[state,]
#Current node is
CurrentNode=which.min(CurrentState[1:n])
#For each state we calculate all the values
OptionsVector=vector(length=n)
#for each option of node to move to calculate the cost and add the previous cost
for(option in 1:n)
{
if(AdjacencyMatrix[CurrentNode,option]==1)
{
OptionsVector[option]=CostOfAction(CurrentState,option,n,CostVec,xVec,LambdaVec) #Cost of action
#We now add the expected future cost, this means a proportion for each possible v state we can transistion to by taking this action
#We can retrive the probability and states from a function
EvolvedStates=NewSVState(CurrentState,option,BVec,bVec,LambdaVec)$State
#We now use the scenario to idenity the evolved state and prob=1
EvolvedState=EvolvedStates[Scenario[CurrentNode,ScenarioTracking[CurrentNode]]]
ScenarioTracking[CurrentNode]=ScenarioTracking[CurrentNode]+1
IDEvolvedState=IdenityRow(EvolvedState,StateSpace)
OptionsVector[option]=OptionsVector[option]+PriorValueFunction[IDEvolvedState]
}
else
{
OptionsVector[option]=NaN
}
}
#Set the Value Vector for that state
ValueVector[state]=min(OptionsVector)
#We will also store the action used to achieve this.
AddOnActionsMatrix[[1,state]]=which(OptionsVector==ValueVector[state])
}
ActionsMatrix=rbind(PriorActionsMatrix,AddOnActionsMatrix)
}
#Return all values
return(list(Values=ValueVector,Actions=ActionsMatrix))
}
ValueFunctionForScenario(1000,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)
ValueFunctionForScenario(2,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)
View(StateSpace)
ValueFunctionForScenario(0,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)
#Function to work out the value for a particular number of steps
#Expects states to be passed as a matrix (with rows being a state with s_1 , s_2,...,s_n,v_1,...,v_2)
ValueFunctionForScenario<-function(Steps,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec,Scenario,ScenarioTracking=NULL,PriorValueFunction=NULL,PriorActionsMatrix=NULL)
{
n=ncol(StateSpace)/2
if(is.null(ScenarioTracking))
{
ScenarioTracking=rep(1,n)
}
StateSpaceSize=nrow(StateSpace)
n=nrow(AdjacencyMatrix)
BVec=ceiling(xVec)
#Stores the value of this iteration
ValueVector=rep(0,StateSpaceSize)
#Store a list of actions for this iteration
AddOnActionsMatrix=matrix(list(),nrow=1,ncol=StateSpaceSize)
if(Steps==0) #Base case
{
return(list(Values=ValueVector,Actions=AddOnActionsMatrix))
}
else if(Steps!=0)
{
#Work out previous step
if(is.null(PriorValueFunction) || is.null(PriorActionsMatrix))
{
Prior=ValueFunction(Steps-1,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec)
PriorValueFunction=Prior$Values
PriorActionsMatrix=Prior$Actions
}
#Form a vector from which to take the minimum for all states
for(state in 1:StateSpaceSize)
{
#Current state is
CurrentState=StateSpace[state,]
#Current node is
CurrentNode=which.min(CurrentState[1:n])
#For each state we calculate all the values
OptionsVector=vector(length=n)
#for each option of node to move to calculate the cost and add the previous cost
for(option in 1:n)
{
if(AdjacencyMatrix[CurrentNode,option]==1)
{
OptionsVector[option]=CostOfAction(CurrentState,option,n,CostVec,xVec,LambdaVec) #Cost of action
#We now add the expected future cost, this means a proportion for each possible v state we can transistion to by taking this action
#We can retrive the probability and states from a function
EvolvedStates=NewSVState(CurrentState,option,BVec,bVec,LambdaVec)$State
#We now use the scenario to idenity the evolved state and prob=1
EvolvedState=EvolvedStates[Scenario[CurrentNode,ScenarioTracking[CurrentNode]]]
print(EvolvedState)
ScenarioTracking[CurrentNode]=ScenarioTracking[CurrentNode]+1
IDEvolvedState=IdenityRow(EvolvedState,StateSpace)
OptionsVector[option]=OptionsVector[option]+PriorValueFunction[IDEvolvedState]
}
else
{
OptionsVector[option]=NaN
}
}
#Set the Value Vector for that state
ValueVector[state]=min(OptionsVector)
#We will also store the action used to achieve this.
AddOnActionsMatrix[[1,state]]=which(OptionsVector==ValueVector[state])
}
ActionsMatrix=rbind(PriorActionsMatrix,AddOnActionsMatrix)
}
#Return all values
return(list(Values=ValueVector,Actions=ActionsMatrix))
}
ValueFunctionForScenario(1,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)
#Function to work out the value for a particular number of steps
#Expects states to be passed as a matrix (with rows being a state with s_1 , s_2,...,s_n,v_1,...,v_2)
ValueFunctionForScenario<-function(Steps,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec,Scenario,ScenarioTracking=NULL,PriorValueFunction=NULL,PriorActionsMatrix=NULL)
{
n=ncol(StateSpace)/2
if(is.null(ScenarioTracking))
{
ScenarioTracking=rep(1,n)
}
StateSpaceSize=nrow(StateSpace)
n=nrow(AdjacencyMatrix)
BVec=ceiling(xVec)
#Stores the value of this iteration
ValueVector=rep(0,StateSpaceSize)
#Store a list of actions for this iteration
AddOnActionsMatrix=matrix(list(),nrow=1,ncol=StateSpaceSize)
if(Steps==0) #Base case
{
return(list(Values=ValueVector,Actions=AddOnActionsMatrix))
}
else if(Steps!=0)
{
#Work out previous step
if(is.null(PriorValueFunction) || is.null(PriorActionsMatrix))
{
Prior=ValueFunction(Steps-1,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec)
PriorValueFunction=Prior$Values
PriorActionsMatrix=Prior$Actions
}
#Form a vector from which to take the minimum for all states
for(state in 1:StateSpaceSize)
{
#Current state is
CurrentState=StateSpace[state,]
#Current node is
CurrentNode=which.min(CurrentState[1:n])
#For each state we calculate all the values
OptionsVector=vector(length=n)
#for each option of node to move to calculate the cost and add the previous cost
for(option in 1:n)
{
if(AdjacencyMatrix[CurrentNode,option]==1)
{
OptionsVector[option]=CostOfAction(CurrentState,option,n,CostVec,xVec,LambdaVec) #Cost of action
#We now add the expected future cost, this means a proportion for each possible v state we can transistion to by taking this action
#We can retrive the probability and states from a function
EvolvedStates=NewSVState(CurrentState,option,BVec,bVec,LambdaVec)$State
#We now use the scenario to idenity the evolved state and prob=1
EvolvedState=EvolvedStates[Scenario[CurrentNode,ScenarioTracking[CurrentNode]],]
print(EvolvedState)
ScenarioTracking[CurrentNode]=ScenarioTracking[CurrentNode]+1
IDEvolvedState=IdenityRow(EvolvedState,StateSpace)
OptionsVector[option]=OptionsVector[option]+PriorValueFunction[IDEvolvedState]
}
else
{
OptionsVector[option]=NaN
}
}
#Set the Value Vector for that state
ValueVector[state]=min(OptionsVector)
#We will also store the action used to achieve this.
AddOnActionsMatrix[[1,state]]=which(OptionsVector==ValueVector[state])
}
ActionsMatrix=rbind(PriorActionsMatrix,AddOnActionsMatrix)
}
#Return all values
return(list(Values=ValueVector,Actions=ActionsMatrix))
}
ValueFunctionForScenario(1,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)
#Function to work out the value for a particular number of steps
#Expects states to be passed as a matrix (with rows being a state with s_1 , s_2,...,s_n,v_1,...,v_2)
ValueFunctionForScenario<-function(Steps,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec,Scenario,ScenarioTracking=NULL,PriorValueFunction=NULL,PriorActionsMatrix=NULL)
{
n=ncol(StateSpace)/2
if(is.null(ScenarioTracking))
{
ScenarioTracking=rep(1,n)
}
StateSpaceSize=nrow(StateSpace)
n=nrow(AdjacencyMatrix)
BVec=ceiling(xVec)
#Stores the value of this iteration
ValueVector=rep(0,StateSpaceSize)
#Store a list of actions for this iteration
AddOnActionsMatrix=matrix(list(),nrow=1,ncol=StateSpaceSize)
if(Steps==0) #Base case
{
return(list(Values=ValueVector,Actions=AddOnActionsMatrix))
}
else if(Steps!=0)
{
#Work out previous step
if(is.null(PriorValueFunction) || is.null(PriorActionsMatrix))
{
Prior=ValueFunction(Steps-1,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec)
PriorValueFunction=Prior$Values
PriorActionsMatrix=Prior$Actions
}
#Form a vector from which to take the minimum for all states
for(state in 1:StateSpaceSize)
{
#Current state is
CurrentState=StateSpace[state,]
#Current node is
CurrentNode=which.min(CurrentState[1:n])
#For each state we calculate all the values
OptionsVector=vector(length=n)
#for each option of node to move to calculate the cost and add the previous cost
for(option in 1:n)
{
if(AdjacencyMatrix[CurrentNode,option]==1)
{
OptionsVector[option]=CostOfAction(CurrentState,option,n,CostVec,xVec,LambdaVec) #Cost of action
#We now add the expected future cost, this means a proportion for each possible v state we can transistion to by taking this action
#We can retrive the probability and states from a function
EvolvedStates=NewSVState(CurrentState,option,BVec,bVec,LambdaVec)$State
#We now use the scenario to idenity the evolved state and prob=1
EvolvedState=EvolvedStates[Scenario[CurrentNode,ScenarioTracking[CurrentNode]],]
ScenarioTracking[CurrentNode]=ScenarioTracking[CurrentNode]+1
print(EvolvedState)
print(StateSpace)
IDEvolvedState=IdenityRow(EvolvedState,StateSpace)
OptionsVector[option]=OptionsVector[option]+PriorValueFunction[IDEvolvedState]
}
else
{
OptionsVector[option]=NaN
}
}
#Set the Value Vector for that state
ValueVector[state]=min(OptionsVector)
#We will also store the action used to achieve this.
AddOnActionsMatrix[[1,state]]=which(OptionsVector==ValueVector[state])
}
ActionsMatrix=rbind(PriorActionsMatrix,AddOnActionsMatrix)
}
#Return all values
return(list(Values=ValueVector,Actions=ActionsMatrix))
}
ValueFunctionForScenario(1,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)
#Function to work out the value for a particular number of steps
#Expects states to be passed as a matrix (with rows being a state with s_1 , s_2,...,s_n,v_1,...,v_2)
ValueFunctionForScenario<-function(Steps,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec,Scenario,ScenarioTracking=NULL,PriorValueFunction=NULL,PriorActionsMatrix=NULL)
{
n=ncol(StateSpace)/2
if(is.null(ScenarioTracking))
{
ScenarioTracking=rep(1,n)
}
StateSpaceSize=nrow(StateSpace)
n=nrow(AdjacencyMatrix)
BVec=ceiling(xVec)
#Stores the value of this iteration
ValueVector=rep(0,StateSpaceSize)
#Store a list of actions for this iteration
AddOnActionsMatrix=matrix(list(),nrow=1,ncol=StateSpaceSize)
if(Steps==0) #Base case
{
return(list(Values=ValueVector,Actions=AddOnActionsMatrix))
}
else if(Steps!=0)
{
#Work out previous step
if(is.null(PriorValueFunction) || is.null(PriorActionsMatrix))
{
Prior=ValueFunction(Steps-1,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec)
PriorValueFunction=Prior$Values
PriorActionsMatrix=Prior$Actions
}
#Form a vector from which to take the minimum for all states
for(state in 1:StateSpaceSize)
{
#Current state is
CurrentState=StateSpace[state,]
#Current node is
CurrentNode=which.min(CurrentState[1:n])
#For each state we calculate all the values
OptionsVector=vector(length=n)
#for each option of node to move to calculate the cost and add the previous cost
for(option in 1:n)
{
if(AdjacencyMatrix[CurrentNode,option]==1)
{
OptionsVector[option]=CostOfAction(CurrentState,option,n,CostVec,xVec,LambdaVec) #Cost of action
#We now add the expected future cost, this means a proportion for each possible v state we can transistion to by taking this action
#We can retrive the probability and states from a function
EvolvedStates=NewSVState(CurrentState,option,BVec,bVec,LambdaVec)$State
#We now use the scenario to idenity the evolved state and prob=1
EvolvedState=EvolvedStates[Scenario[CurrentNode,ScenarioTracking[CurrentNode]]+1,]
ScenarioTracking[CurrentNode]=ScenarioTracking[CurrentNode]+1
print(EvolvedState)
print(StateSpace)
IDEvolvedState=IdenityRow(EvolvedState,StateSpace)
OptionsVector[option]=OptionsVector[option]+PriorValueFunction[IDEvolvedState]
}
else
{
OptionsVector[option]=NaN
}
}
#Set the Value Vector for that state
ValueVector[state]=min(OptionsVector)
#We will also store the action used to achieve this.
AddOnActionsMatrix[[1,state]]=which(OptionsVector==ValueVector[state])
}
ActionsMatrix=rbind(PriorActionsMatrix,AddOnActionsMatrix)
}
#Return all values
return(list(Values=ValueVector,Actions=ActionsMatrix))
}
ValueFunctionForScenario(1,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)
View(Scenario)
Scenario=CreateSimulationScenario(1000,3,rep(1/3,3))
#Function to work out the value for a particular number of steps
#Expects states to be passed as a matrix (with rows being a state with s_1 , s_2,...,s_n,v_1,...,v_2)
ValueFunctionForScenario<-function(Steps,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec,Scenario,ScenarioTracking=NULL,PriorValueFunction=NULL,PriorActionsMatrix=NULL)
{
n=ncol(StateSpace)/2
if(is.null(ScenarioTracking))
{
ScenarioTracking=rep(1,n)
}
StateSpaceSize=nrow(StateSpace)
n=nrow(AdjacencyMatrix)
BVec=ceiling(xVec)
#Stores the value of this iteration
ValueVector=rep(0,StateSpaceSize)
#Store a list of actions for this iteration
AddOnActionsMatrix=matrix(list(),nrow=1,ncol=StateSpaceSize)
if(Steps==0) #Base case
{
return(list(Values=ValueVector,Actions=AddOnActionsMatrix))
}
else if(Steps!=0)
{
#Work out previous step
if(is.null(PriorValueFunction) || is.null(PriorActionsMatrix))
{
Prior=ValueFunction(Steps-1,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec)
PriorValueFunction=Prior$Values
PriorActionsMatrix=Prior$Actions
}
#Form a vector from which to take the minimum for all states
for(state in 1:StateSpaceSize)
{
#Current state is
CurrentState=StateSpace[state,]
#Current node is
CurrentNode=which.min(CurrentState[1:n])
#For each state we calculate all the values
OptionsVector=vector(length=n)
#for each option of node to move to calculate the cost and add the previous cost
for(option in 1:n)
{
if(AdjacencyMatrix[CurrentNode,option]==1)
{
OptionsVector[option]=CostOfAction(CurrentState,option,n,CostVec,xVec,LambdaVec) #Cost of action
#We now add the expected future cost, this means a proportion for each possible v state we can transistion to by taking this action
#We can retrive the probability and states from a function
EvolvedStates=NewSVState(CurrentState,option,BVec,bVec,LambdaVec)$State
#We now use the scenario to idenity the evolved state and prob=1
EvolvedState=EvolvedStates[(min(bVec[CurrentNode],Scenario[CurrentNode,ScenarioTracking[CurrentNode]])+1),]
ScenarioTracking[CurrentNode]=ScenarioTracking[CurrentNode]+1
IDEvolvedState=IdenityRow(EvolvedState,StateSpace)
OptionsVector[option]=OptionsVector[option]+PriorValueFunction[IDEvolvedState]
}
else
{
OptionsVector[option]=NaN
}
}
#Set the Value Vector for that state
ValueVector[state]=min(OptionsVector)
#We will also store the action used to achieve this.
AddOnActionsMatrix[[1,state]]=which(OptionsVector==ValueVector[state])
}
ActionsMatrix=rbind(PriorActionsMatrix,AddOnActionsMatrix)
}
#Return all values
return(list(Values=ValueVector,Actions=ActionsMatrix))
}
Scenario=CreateSimulationScenario(1000,3,rep(1/3,3))
ValueFunctionForScenario(1,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)
ValueFunctionForScenario(10,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)
ValueFunctionForScenario(100,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)
ValueFunctionForScenario(1000,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)
min(ValueFunctionForScenario(100,StateSpace,matrix(rep(1,9),nrow=3),c(1.2,1.8,2.2),rep(2,3),rep(1,3),rep(1/3,3),Scenario)$Values)
FullInfo=SimulationForEvolution(100,3,MultiStepPenaltyHeuristic,3,matrix(rep(1,9),nrow=3),PlainIndexForNode,rep(1,3),rep(1/3,3),rep(2,3),c(1.2,1.8,2.2),Scenario)$FullInfoMatrix
CompareSimulationInfoToPolicy(FullInfo,PolicyByDual,StateSpace,c(2,2,3))
FullInfo=SimulationForEvolution(1000,3,MultiStepPenaltyHeuristic,3,matrix(rep(1,9),nrow=3),PlainIndexForNode,rep(1,3),rep(1/3,3),rep(2,3),c(1.2,1.8,2.2),Scenario)$FullInfoMatrix
CompareSimulationInfoToPolicy(FullInfo,PolicyByDual,StateSpace,c(2,2,3))
FullInfo=SimulationForEvolution(1000,3,MultiStepPenaltyHeuristic,3,matrix(rep(1,9),nrow=3),PlainIndexForNode,rep(1,3),rep(1/3,3),rep(2,3),c(1.2,1.8,2.2),Scenario)$FullInfoMatrix
0.005/0.15
