}
}
ALHS=rbind(ALHS,ConstraintRow)
bRHS=c(bRHS,0)
}
#SECOND DUAL CONSTRAINT
for(StateNumber in 1:nrow(SVStateSpace))
{
#We will now create the constraint for taking that action
ConstraintRow=vector(length=NumberOfVariables)
#We store our state we are working on the constraint row for
State=SVStateSpace[StateNumber,]
CurrentNode=which.min(State[1:n])
CurrentNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
Actions=vector(length=0)
for(Node in 1:n)
{
if(CurrentNodeRow[Node]==1)
{
Actions=c(Actions,Node)
}
}
#We place 1's in the all the actions possible for this state for x
for(i in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
ConstraintRow[i]=ConstraintRow[i]+1
}
else
{
ConstraintRow[sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]=
ConstraintRow[sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]+1
}
}
#We place 1's in the all the actions possible for this state for y
for(i in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
ConstraintRow[NumberOfXVariables+i]=ConstraintRow[NumberOfXVariables+i]+1
}
else
{
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]=
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]+1
}
}
#We now subtract prob(moving to state summed over all states and actions) for y
for(oldstatenumber in 1:nrow(SVStateSpace))
{
OldState=SVStateSpace[oldstatenumber,]
OldNode=which.min(OldState[1:n])
OldNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
OldActions=vector(length=0)
for(Node in 1:n)
{
if(OldNodeRow[Node]==1)
{
OldActions=c(OldActions,Node)
}
}
for(actionnumber in 1:NumberOfActionsFromState[oldstatenumber])
{
#Using this oldstate and action , is it possible that the new state is the current working state
New=NewSVState(OldState,OldActions[actionnumber],BVec,bVec,LambdaVec)
NewState=New$State
NewStateProb=New$Prob
for(NewStateNumber in 1:nrow(NewState))
{
if(all(NewState[NewStateNumber,]==SVStateSpace[StateNumber,]))
{
#If we can possibly move to the state we are working on we subtract the probability
if(oldstatenumber==1)
{
ConstraintRow[NumberOfXVariables+actionnumber]=ConstraintRow[NumberOfXVariables+actionnumber]-NewStateProb[NewStateNumber]
}
else
{
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(oldstatenumber-1)])+actionnumber]=
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(oldstatenumber-1)])+actionnumber]-NewStateProb[NewStateNumber]
}
}
}
}
}
ALHS=rbind(ALHS,ConstraintRow)
bRHS=c(bRHS,AlphaVec[StateNumber])
}
Objective=vector(length=NumberOfVariables)
#We now create the Objective
for(StateNumber in 1:nrow(SVStateSpace))
{
#We store our state we are working on the constraint row for
State=SVStateSpace[StateNumber,]
CurrentNode=which.min(State[1:n])
CurrentNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
Actions=vector(length=0)
for(Node in 1:n)
{
if(CurrentNodeRow[Node]==1)
{
Actions=c(Actions,Node)
}
}
for(ActionNumber in 1:NumberOfActionsFromState[StateNumber])
{
#For this state and this action store the cost in the objective function
if(StateNumber==1)
{
Objective[ActionNumber]=CostOfAction(State,Actions[ActionNumber],n,CostVec,xVec,LambdaVec)
}
else
{
Objective[sum(NumberOfActionsFromState[1:StateNumber-1])+ActionNumber]=CostOfAction(State,Actions[ActionNumber],n,CostVec,xVec,LambdaVec)
}
}
}
return(list(Objective=Objective,MatrixConstraints=ALHS,VectorBounds=bRHS,StateSpace=SVStateSpace,NumberOfActionsFromState=NumberOfActionsFromState,AlphaVec=AlphaVec))
}
SolveDualLP(matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),rep(1/4,4))
#We Note our list of x,y are in the form of blocks; x->state->action
CreateDualSetup<-function(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec,SVStateSpace=NULL,AlphaVec=NULL)
{
BVec=ceiling(xVec)
if(is.null(SVStateSpace))
{
SVStateSpace=CreateSVStates(n,BVec,bVec)
}
if(is.null(AlphaVec))
{
AlphaVec=vector(length=nrow(SVStateSpace))
AlphaVec=rep(1/length(AlphaVec),length(AlphaVec))
}
#We now work out the number of x's and y's needed (i.e the size of (s,a))
NumberOfXVariables=0
NumberOfActionsFromState=vector(length=0)
for(state in 1:nrow(SVStateSpace))
{
#For each state we need to work out the number of actions
#First we work out the current node (and then how many actions can be taken)
StateVector=SVStateSpace[state,]
CurrentNode=which.min(StateVector[1:n])
#print(paste("Current Node is ",toString(CurrentNode)))
AdjRow=AdjMatrix[CurrentNode,]
NumberOfXVariables=NumberOfXVariables+length(AdjRow[AdjRow==1])
NumberOfActionsFromState=c(NumberOfActionsFromState,length(AdjRow[AdjRow==1]))
#print(NumberOfActionsFromState)
}
NumberOfYVariables=NumberOfXVariables
NumberOfVariables=NumberOfXVariables+NumberOfYVariables
#creating the LHS matrix part A
ALHS=matrix(nrow=0,ncol=NumberOfVariables)
#storage for the RHS vector part b
bRHS=vector(length=0)
#FIRST DUAL CONSTRAINT
for(StateNumber in 1:nrow(SVStateSpace))
{
#We will now create the constraint for taking that action
ConstraintRow=vector(length=NumberOfVariables)
#We store our state we are working on the constraint row for
State=SVStateSpace[StateNumber,]
CurrentNode=which.min(State[1:n])
CurrentNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
Actions=vector(length=0)
for(Node in 1:n)
{
if(CurrentNodeRow[Node]==1)
{
Actions=c(Actions,Node)
}
}
#We place 1's in the all the actions possible for this state x
for(i in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
ConstraintRow[i]=ConstraintRow[i]+1
}
else
{
ConstraintRow[sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]=
ConstraintRow[sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]+1
}
}
#We now subtract prob(moving to state summed over all states and actions) for x
for(oldstatenumber in 1:nrow(SVStateSpace))
{
OldState=SVStateSpace[oldstatenumber,]
OldNode=which.min(OldState[1:n])
OldNodeRow=AdjMatrix[OldNode,]
#Actions available from this state
OldActions=vector(length=0)
for(Node in 1:n)
{
if(OldNodeRow[Node]==1)
{
OldActions=c(OldActions,Node)
}
}
for(actionnumber in 1:NumberOfActionsFromState[oldstatenumber])
{
#Using this oldstate and action , is it possible that the new state is the current working state
New=NewSVState(OldState,OldActions[actionnumber],BVec,bVec,LambdaVec)
NewState=New$State
NewStateProb=New$Prob
for(NewStateNumber in 1:nrow(NewState))
{
if(all(NewState[NewStateNumber,]==SVStateSpace[StateNumber,]))
{
#If we can possibly move to the state we are working on we subtract the probability
if(oldstatenumber==1)
{
ConstraintRow[actionnumber]=ConstraintRow[actionnumber]-NewStateProb[NewStateNumber]
}
else
{
ConstraintRow[sum(NumberOfActionsFromState[1:(oldstatenumber-1)])+actionnumber]=
ConstraintRow[sum(NumberOfActionsFromState[1:(oldstatenumber-1)])+actionnumber]-NewStateProb[NewStateNumber]
}
}
}
}
}
ALHS=rbind(ALHS,ConstraintRow)
bRHS=c(bRHS,0)
}
#SECOND DUAL CONSTRAINT
for(StateNumber in 1:nrow(SVStateSpace))
{
#We will now create the constraint for taking that action
ConstraintRow=vector(length=NumberOfVariables)
#We store our state we are working on the constraint row for
State=SVStateSpace[StateNumber,]
CurrentNode=which.min(State[1:n])
CurrentNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
Actions=vector(length=0)
for(Node in 1:n)
{
if(CurrentNodeRow[Node]==1)
{
Actions=c(Actions,Node)
}
}
#We place 1's in the all the actions possible for this state for x
for(i in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
ConstraintRow[i]=ConstraintRow[i]+1
}
else
{
ConstraintRow[sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]=
ConstraintRow[sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]+1
}
}
#We place 1's in the all the actions possible for this state for y
for(i in 1:NumberOfActionsFromState[StateNumber])
{
if(StateNumber==1)
{
ConstraintRow[NumberOfXVariables+i]=ConstraintRow[NumberOfXVariables+i]+1
}
else
{
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]=
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(StateNumber-1)])+i]+1
}
}
#We now subtract prob(moving to state summed over all states and actions) for y
for(oldstatenumber in 1:nrow(SVStateSpace))
{
OldState=SVStateSpace[oldstatenumber,]
OldNode=which.min(OldState[1:n])
OldNodeRow=AdjMatrix[OldNode,]
#Actions available from this state
OldActions=vector(length=0)
for(Node in 1:n)
{
if(OldNodeRow[Node]==1)
{
OldActions=c(OldActions,Node)
}
}
for(actionnumber in 1:NumberOfActionsFromState[oldstatenumber])
{
#Using this oldstate and action , is it possible that the new state is the current working state
New=NewSVState(OldState,OldActions[actionnumber],BVec,bVec,LambdaVec)
NewState=New$State
NewStateProb=New$Prob
for(NewStateNumber in 1:nrow(NewState))
{
if(all(NewState[NewStateNumber,]==SVStateSpace[StateNumber,]))
{
#If we can possibly move to the state we are working on we subtract the probability
if(oldstatenumber==1)
{
ConstraintRow[NumberOfXVariables+actionnumber]=ConstraintRow[NumberOfXVariables+actionnumber]-NewStateProb[NewStateNumber]
}
else
{
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(oldstatenumber-1)])+actionnumber]=
ConstraintRow[NumberOfXVariables+sum(NumberOfActionsFromState[1:(oldstatenumber-1)])+actionnumber]-NewStateProb[NewStateNumber]
}
}
}
}
}
ALHS=rbind(ALHS,ConstraintRow)
bRHS=c(bRHS,AlphaVec[StateNumber])
}
Objective=vector(length=NumberOfVariables)
#We now create the Objective
for(StateNumber in 1:nrow(SVStateSpace))
{
#We store our state we are working on the constraint row for
State=SVStateSpace[StateNumber,]
CurrentNode=which.min(State[1:n])
CurrentNodeRow=AdjMatrix[CurrentNode,]
#Actions available from this state
Actions=vector(length=0)
for(Node in 1:n)
{
if(CurrentNodeRow[Node]==1)
{
Actions=c(Actions,Node)
}
}
for(ActionNumber in 1:NumberOfActionsFromState[StateNumber])
{
#For this state and this action store the cost in the objective function
if(StateNumber==1)
{
Objective[ActionNumber]=CostOfAction(State,Actions[ActionNumber],n,CostVec,xVec,LambdaVec)
}
else
{
Objective[sum(NumberOfActionsFromState[1:StateNumber-1])+ActionNumber]=CostOfAction(State,Actions[ActionNumber],n,CostVec,xVec,LambdaVec)
}
}
}
return(list(Objective=Objective,MatrixConstraints=ALHS,VectorBounds=bRHS,StateSpace=SVStateSpace,NumberOfActionsFromState=NumberOfActionsFromState,AlphaVec=AlphaVec))
}
SolveDualLP(matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),rep(1/4,4))
#Evolution of S states function
NewSState<-function(CurrentSState,NodeMovedTo,BVec)
{
NewSState=vector(length=length(CurrentSState))
for(i in 1:length(CurrentSState))
{
if(i==NodeMovedTo)
{
NewSState[i]=1
}
else
{
NewSState[i]=min(CurrentSState[i]+1,BVec[i]+1)
}
}
return(NewSState)
}
PolicyForTest=HeuristicPolicy(4,MultiStepBenefitHeuristic,4,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),EqualStepIndexForNode,rep(1,4),rep(1/4,4),rep(2,4),c(1.2,1.8,2.2,2.8))
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(2,4),rep(1,4),rep(1/4,4),PolicyForTest)
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),PlainIndexForNode,rep(1,4),rep(1/4,4),rep(2,4),c(1.2,1.8,2.2,2.8))
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(2,4),rep(1,4),rep(1/4,4),PolicyForTest)
SolveDualLP(matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),rep(1/4,4))
SolveDualLP(matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4))
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),PlainIndexForNode,rep(1,4),rep(1/4,4),rep(0,4),c(1.2,1.8,2.2,2.8))
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(2,4),rep(1,4),rep(1/4,4),PolicyForTest)
SolveDualLP(matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4))
StateSpaceTest=CreateSVStates(4,c(2,2,3,3),rep(0,4))
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),PlainIndexForNode,rep(1,4),rep(1/4,4),rep(0,4),c(1.2,1.8,2.2,2.8))
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),PlainIndexForNode,rep(1,4),rep(1/4,4),rep(0,4),c(1.2,1.8,2.2,2.8))
PolicyForTest
StateSpaceTest
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueFunctionForPolicy(5,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueFunctionForPolicy(5,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueFunctionForPolicy(0,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueFunctionForPolicy(1,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueFunctionForPolicy(2,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
min(ValueFunctionForPolicy(2,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest))
ValueFunctionForPolicy(3,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
StateSpaceTest
PolicyForTest
FindOptimalEquilibriumValuesByDual(matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),4,c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4))
ValueFunctionForPolicy(30,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueFunctionForPolicy(100,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueFunctionForPolicy(1000,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
PolicyForTest
SolveDualLP(matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4,byrow = T),4,c(2.2,2.8,3.2,3.8),rep(0,4),rep(1,4),rep(1/4,4))
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),PlainIndexForNode,rep(1,4),rep(1/4,4),rep(0,4),c(2.2,2.8,3.2,3.8))
ValueFunctionForPolicy(100,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(2.2,2.8,3.2,3.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
StateSpaceTest=CreateSVStates(4,c(3,3,4,4),rep(0,4))
ValueFunctionForPolicy(100,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(2.2,2.8,3.2,3.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(2.2,2.8,3.2,3.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4))
StateSpaceTest=CreateSVStates(4,c(2,2,3,3),rep(0,4))
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4),PlainIndexForNode,rep(1,4),rep(1/4,4),rep(0,4),c(1.2,1.8,2.2,2.8))
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(c(1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4),PlainIndexForNode,rep(1,4),rep(1/4,4),rep(0,4),c(1.2,1.8,2.2,2.8))
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4))
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),rep(1/4,4))
PolicyForTest=HeuristicPolicy(3,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4),PlainIndexForNode,rep(1,4),rep(1/4,4),rep(1,4),c(1.2,1.8,2.2,2.8))
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),rep(1/4,4),PolicyForTest)
StateSpaceTest=CreateSVStates(4,c(2,2,3,3),rep(1,4))
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),rep(1/4,4),PolicyForTest)
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),rep(1/4,4))
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4),PlainIndexForNode,rep(1,4),rep(1/4,4),rep(1,4),c(1.2,1.8,2.2,2.8))
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),rep(1/4,4),PolicyForTest)
colourpicker:::colourPickerAddin()
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4))
library(gtools)
library(utils)
library(lpSolve)
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4))
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4),PlainIndexForNode,rep(1,4),rep(1/4,4),rep(0,4),c(1.2,1.8,2.2,2.8))
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
StateSpaceTest=CreateSVStates(4,c(2,2,3,3),rep(0,4))
ValueIterationForPolicy(500,0.00001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueFunctionForPolicy(100,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.3,0.1,0.4))
StateSpaceTest=CreateSVStates(4,c(2,2,3,3),rep(1,4))
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4),PlainIndexForNode,rep(1,4),c(0.2,0.3,0.1,0.4),rep(0,4),c(1.2,1.8,2.2,2.8))
ValueFunctionForPolicy(100,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(0,4),rep(1,4),rep(1/4,4),PolicyForTest)
ValueFunctionForPolicy(100,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.3,0.1,0.4),PolicyForTest)
ValueFunctionForPolicy(100,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.3,0.1,0.4),PolicyForTest)
PolicyForTest
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4),PlainIndexForNode,rep(1,4),c(0.2,0.3,0.1,0.4),rep(1,4),c(1.2,1.8,2.2,2.8))
ValueFunctionForPolicy(100,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.3,0.1,0.4),PolicyForTest)
ValueIterationForPolicy(100,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.3,0.1,0.4),PolicyForTest)
ValueIterationForPolicy(100,0.001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.3,0.1,0.4),PolicyForTest)
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.3,0.1,0.4))
ValueIterationForPolicy(100,0.001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.3,0.1,0.4),PolicyForTest)
14/194
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.1,0.1,0.1,0.7))
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4),PlainIndexForNode,rep(1,4),c(0.1,0.1,0.1,0.7),rep(1,4),c(1.2,1.8,2.2,2.8))
ValueIterationForPolicy(100,0.001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.1,0.1,0.1,0.7),PolicyForTest)
ValueIterationForPolicy(500,0.001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.1,0.1,0.1,0.7),PolicyForTest)
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.1,0.1,0.1,0.7))
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.2,0.2,0.4))
PolicyForTest=HeuristicPolicy(4,MultiStepPenaltyHeuristic,4,matrix(rep(1,16),nrow=4),PlainIndexForNode,rep(1,4),c(0.2,0.2,0.2,0.4),rep(1,4),c(1.2,1.8,2.2,2.8))
ValueIterationForPolicy(500,0.001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.2,0.2,0.4),PolicyForTest)
SolveDualLP(matrix(rep(1,16),nrow=4,byrow = T),4,c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.2,0.2,0.4))
ValueIterationForPolicy(500,0.001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.2,0.2,0.4),PolicyForTest)
ValueIterationForPolicy(500,0.0001,StateSpaceTest,matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.2,0.2,0.4),PolicyForTest)
0.0004/0.26
SolveDualLP<-function(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec)
{
print("Starting to Set up the dual problem")
CreatedDual=CreateDualSetup(AdjMatrix,n,xVec,bVec,CostVec,LambdaVec)
A=CreatedDual$MatrixConstraints
b=CreatedDual$VectorBounds
StateSpace=CreatedDual$StateSpace
print(A)
print(b)
Objdir="min"
Objective=CreatedDual$Objective
Constdir=rep("=",nrow(A))
print("Starting to solve")
Solved=lp(Objdir,Objective,A,Constdir,b)
return(list(Value=Solved ,Solution=Solved$solution,StateSpace=StateSpace))
}
RunTest<-function(AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec,HeuristicFunction,HeuristicDepth,IndexForNodeFunction,MaxStepsForIteration)
{
n=nrow(AdjacencyMatrix)
#We first solve for optimality using the dual
DualSolved=SolveDualLP(AdjacencyMatrix,n,xVec,bVec,CostVec,LambdaVec)
DualObjectiveValue=DualSolved$Solution
#We now create the Heuristic policy
PolicyByHeuristic=HeuristicPolicy(HeuristicDepth,HeuristicFunction,AdjacencyMatrix,IndexForNodeFunction,CostVec,LambdaVec,bVec,xVec)
#Run the Heurstic policy
#We will select the tolerance to depend on the answer above, because we care about the size of the error we will work to a minimum error of 0.01% so that is we care about a tenthousandth error
ToleranceForIt=10^floor(log10(DualObjectiveValue))
StateSpace=DualSolved$StateSpace
ValueItByHeuristic=ValueIterationForPolicy(MaxStepsForIteration,ToleranceForIt,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec,PolicyByHeuristic)
#We now work out the level of error and return it
AbsError=ValueItByHeuristic$UpperBound - DualObjectiveValue
PercentageError=(AbsError/DualObjectiveValue) *100
return(PercentageError)
}
RunTest(matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.2,0.2,0.4),MultiStepPenaltyHeuristic,3,PlainIndexForNode,1000)
RunTest<-function(AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec,HeuristicFunction,HeuristicDepth,IndexForNodeFunction,MaxStepsForIteration)
{
n=nrow(AdjacencyMatrix)
#We first solve for optimality using the dual
DualSolved=SolveDualLP(AdjacencyMatrix,n,xVec,bVec,CostVec,LambdaVec)
DualObjectiveValue=DualSolved$Solution
#We now create the Heuristic policy
PolicyByHeuristic=HeuristicPolicy(HeuristicDepth,n,HeuristicFunction,AdjacencyMatrix,IndexForNodeFunction,CostVec,LambdaVec,bVec,xVec)
#Run the Heurstic policy
#We will select the tolerance to depend on the answer above, because we care about the size of the error we will work to a minimum error of 0.01% so that is we care about a tenthousandth error
ToleranceForIt=10^floor(log10(DualObjectiveValue))
StateSpace=DualSolved$StateSpace
ValueItByHeuristic=ValueIterationForPolicy(MaxStepsForIteration,ToleranceForIt,StateSpace,AdjacencyMatrix,xVec,bVec,CostVec,LambdaVec,PolicyByHeuristic)
#We now work out the level of error and return it
AbsError=ValueItByHeuristic$UpperBound - DualObjectiveValue
PercentageError=(AbsError/DualObjectiveValue) *100
return(PercentageError)
}
RunTest(matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.2,0.2,0.4),MultiStepPenaltyHeuristic,3,PlainIndexForNode,1000)
RunTest(matrix(rep(1,16),nrow=4),c(1.2,1.8,2.2,2.8),rep(1,4),rep(1,4),c(0.2,0.2,0.2,0.4),MultiStepPenaltyHeuristic,3,PlainIndexForNode,1000)
